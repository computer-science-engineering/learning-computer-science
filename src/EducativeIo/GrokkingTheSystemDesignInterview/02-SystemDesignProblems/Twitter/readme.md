# Designing Twitter

- [Designing Twitter](#designing-twitter)
  - [What is Twitter](#what-is-twitter)
  - [Requirements and Goals of the System](#requirements-and-goals-of-the-system)
    - [Functional Requirements](#functional-requirements)
    - [Non-Functional Requirements](#non-functional-requirements)
    - [Extended Requirements](#extended-requirements)
  - [Capacity Estimation and Constraints](#capacity-estimation-and-constraints)
    - [Storage Estimates](#storage-estimates)
    - [Bandwidth Estimates](#bandwidth-estimates)
  - [System APIs](#system-apis)
  - [High Level System Design](#high-level-system-design)
  - [Database Schema](#database-schema)
    - [Tables](#tables)
      - [Tweet](#tweet)
      - [User](#user)
      - [UserFollow](#userfollow)
      - [Favorite](#favorite)
    - [Type of Database](#type-of-database)
  - [Data Sharding](#data-sharding)
    - [Sharding based on UserID](#sharding-based-on-userid)
    - [Sharding based on TweetID](#sharding-based-on-tweetid)
    - [Sharding based on Tweet creation time](#sharding-based-on-tweet-creation-time)
    - [Sharding based on TweetID and Tweet creation time](#sharding-based-on-tweetid-and-tweet-creation-time)
  - [Cache](#cache)
  - [Timeline Generation](#timeline-generation)
  - [Replication and Fault Tolerance](#replication-and-fault-tolerance)
  - [Load Balancing](#load-balancing)
  - [Monitoring](#monitoring)
  - [Extended Requirements](#extended-requirements-1)

Difficulty Level: Medium

## What is Twitter

## Requirements and Goals of the System

### Functional Requirements

1. Users should be able to post new tweets.
2. A user should be able to follow other users.
3. Users should be able to mark tweets as favorites.
4. The service should be able to create and display a userâ€™s timeline consisting of top tweets from all the people the user follows.
5. Tweets can contain photos and videos.

### Non-Functional Requirements

1. Our service needs to be highly available.
2. Acceptable latency of the system is 200ms for timeline generation.
3. Consistency can take a hit (in the interest of availability); if a user doesn't see a tweet for a while, it should be fine.

### Extended Requirements

## Capacity Estimation and Constraints

- 1 B total users with 200 M DAU.
- 100 M new tweets every day.
- On average each user follows 200 people.
- Favorites per day
  - Say, each user favorites 5 tweets per day.
  - `200 M users * 5 favorites = 1 B`
- Total tweet-views generated by system
  - Say, user visits timeline twice a day, and visits 5 other people's pages.
  - On each page say user sees 20 tweets
  - Total tweet-views = `200 M DAU * ((2+5)* 20 tweets) = 28 B /day`

### Storage Estimates

```text
Each tweet has 140 characters
To store each character without compression we need 2 bytes
Metadata for each tweet (ID, timestamp,userID, etc.), say we need 30 bytes
Total storage needed = 100 M tweets/day * (280 + 30) bytes = 30 GB/day

Say, on average every fifth tweet has a photo and every tenth tweet has a video.
Say, on average a photo is 200 KB and a video is 2 MB.
Storage needed for new media every day
  = (100M/5 photos * 200 KB) + (100M/10 videos * 2 MB) ~= 24 TB/day
```

### Bandwidth Estimates

```text
Total ingress =  24 TB/day = 290 MB/sec
Tweet-views per day = 28 B
Say, users will only watch 3rd video they see in their timeline
Total egress
  = (28 B * 280 bytes) / 86400s of text => 93 MB/sec
    + (28B/5 * 200 KB) / 86400s of photos => 13 GB/sec
    + (28B/10/3 * 2 MB) / 86400s of videos => 22 GB/sec
  ~= 35 GB/sec
```

## System APIs

```text
tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids, maximum_results_to_return)

returns URL to access the tweet.
```

## High Level System Design

- Need to store 100M/86400s = 1150 tweets/sec and read 28B/86400s = 325K tweets/sec.
- Read heavy system.
- This traffic will be distributed unevenly throughout the day, though, at peak time we should expect at least a few thousand write requests and around 1M read requests per second.

![high level design](https://raw.githubusercontent.com/tuliren/grokking-system-design/master/img/twitter-overview.png)

## Database Schema

### Tables

#### Tweet

| Column         | Type         |
| -------------- | ------------ |
| TweetID (PK)   | int          |
| UserID         | int          |
| Content        | varchar(140) |
| TweetLatitude  | int          |
| TweetLongitude | int          |
| UserLatitude   | int          |
| UserLongitude  | int          |
| CreationDate   | datetime     |
| NumFavorites   | int          |


#### User

| Column       | Type        |
| ------------ | ----------- |
| UserID (PK)  | int         |
| Name         | varchar(20) |
| Email        | varchar(32) |
| DateOfBirth  | datetime    |
| CreationDate | datetime    |
| LastLogin    | datetime    |

#### UserFollow

| Column       | Type |
| ------------ | ---- |
| UserID1 (PK) | int  |
| UserID2 (PK) | int  |

#### Favorite

| Column       | Type     |
| ------------ | -------- |
| TweetID (PK) | int      |
| UserID (PK)  | int      |
| CreationDate | datetime |

### Type of Database

- Obvious choice is to use an RDBMS d/b, but scaling can be difficult.
- For scaling advantage of NoSQL, a distributed K-V store can be used.
  - Metadata related to photos can go to a table where key would be PhotoID and value would be an object containing rest of the items.
- Column-wide datastore like Cassandra can be used for
  - Storing relationships between users and photos.
    - UserPhoto table: key would be UserID, value would be a list of PhotoIDs the user owns, stored in different columns.
  - List of people a user follows.
    - UserFollow table: Similar as for UserPhoto table.
- K-V stores maintain replicas for reliability. Also data is marked as deleted before getting deleted permanently.
- Photos can be stored on a distributed file storage like HDFS or S3, or Azure blobs.

## Data Sharding

- Read load is extremely high.

### Sharding based on UserID

- All data for a user on one server.
- Hash function uses UserID to map user to d/b server which stores all user's tweets, favorites, follows, etc.
- Issues
  - Could lead to hot-spots, if a user becomes hot.
  - Maintaining uniform distribution of growing user data is difficult.
- Solution
  - Repartition/redistribute data, or use consistent hashing.

### Sharding based on TweetID

- Hash function will map TweetID to a server where tweet will be stored.
- Timeline generation workflow
  1. Our application (app) server will find all the people the user follows.
  2. App server will send the query to all database servers to find tweets from these people.
  3. Each database server will find the tweets for each user, sort them by recency and return the top tweets.
  4. App server will merge all the results and sort them again to return the top results to the user.
- Issues
  - High latencies due to having to query multiple d/b partitions and do aggregation.
- Solution
  - Cache to store hot tweets in front of database servers.

### Sharding based on Tweet creation time

- Will be able to fetch all top tweets quickly.
- Will have to query only a very small set of servers.
- Issues
  - Traffic load will not be distributed.
  - While writing, new tweets will go to one server, while remaining servers will be idle.
  - While reading, server holding latest data will have high load.

### Sharding based on TweetID and Tweet creation time

- Make TweetID universally unique and each TweetID will contain a timestamp.
- Using epoch time.
- TweetID will have 2 parts.
  - First part will be representing epoch seconds.
  - Second part will be an auto-incrementing sequence.
- We can take the current epoch time and append an auto-incrementing number to it.
- We can figure out the shard number from this TweetID and store it there.

## Cache

## Timeline Generation

## Replication and Fault Tolerance

## Load Balancing

## Monitoring

## Extended Requirements